One tradeoff between UTF-32 and UTF-8 is space and efficiency. Since all Unicode points in UTF-32 are 4 bytes, using UTF-32 would take up 4x more memory than UTF-8. If we wanted to represent an ASCII character in UTF-8 like 'a', it would take 1 byte but in UTF-32, it would take 4 bytes. Using more memory could result in slower processing.

The leading 10 could be useful in differentiating the size of bytes. If the coding for 3 bytes followed the format 1110XXXX XXXXXXXX XXXXXXXX, it could be difficult to differentiate between bytes. XXXXXXXX could be a regular ASCII byte. If I were to look at a series of bytes, I would have to search through previous bytes to see if the XXXXXXXX byte that I was looking at was a continuation byte or just a single byte. It could also lead to a lot of bugs. I think that memory leaks could be more common since it is hard to differentiate if the byte is a continuation byte or a single byte. It seems like it would be more likely for programs to run on in memory like how the arary in class did when there was no ending zero. It could also end early since XXXXXXXX could match another starting byte for a 4 byte sequence. 